{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069498c9",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# \u00ad\u0192\u00dc\u00bf MASTER SETUP BLOCK - DO NOT EDIT THIS CELL\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# \u00ad\u0192\u00dc\u00c7 MASTER DATA PIPELINE (LOAD -> CLEAN -> FILL)\n",
    "# ==============================================================================\n",
    "# Instructions:\n",
    "# 1. Place 'US_Accidents_March23.csv' in a folder named 'data'.\n",
    "# 2. Run the cells below in order to get a clean dataframe ready for analysis.\n",
    "# ==============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# \u00ad\u0192\u00f2\u00c1 DIRTY EDA (Audit the Raw Data)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. LOAD RAW DATA\n",
    "file_path = os.path.join('data', 'US_Accidents.csv')\n",
    "print(\"\u23f3 Loading raw data...\")\n",
    "df_raw = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c397fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adfca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MINIMAL TYPE FIX (Required for plotting, but NO data deleted)\n",
    "df_raw['Start_Time'] = pd.to_datetime(df_raw['Start_Time'], errors='coerce')\n",
    "df_raw['Hour'] = df_raw['Start_Time'].dt.hour  # Create Hour just for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad969ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. VISUALIZE THE MESS (Missing Values)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_raw.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title(\"Missing Value Map (Yellow = Missing)\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58ecd1",
   "metadata": {},
   "source": [
    " *Insight:* Look at the vertical yellow lines. Those are columns you should probably delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CHECK FOR OUTLIERS (e.g., Temperature)\n",
    "# ---------------------------------------------------------\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.boxplot(x=df_raw['Temperature(F)'])\n",
    "plt.title(\"Temperature Outliers Check\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f873fb",
   "metadata": {},
   "source": [
    "*Insight:* If you see dots at -100 or +200, you know you need to filter them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4810ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. CHECK TARGET BALANCE (Severity)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n--- Severity Counts (Raw) ---\")\n",
    "print(df_raw['Severity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2611f",
   "metadata": {},
   "source": [
    "# Cleaning the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddc7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "cols_to_delete = [\n",
    "    # Metadata & IDs\n",
    "    'ID', 'Source', 'Country', 'Zipcode', 'Timezone', 'Airport_Code', 'Description',\n",
    "\n",
    "    # Redundant Time/Weather\n",
    "    'Weather_Timestamp', 'Wind_Chill(F)', 'Wind_Direction', 'Pressure(in)', 'Precipitation(in)',\n",
    "    'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight',\n",
    "\n",
    "    # Too Granular or Mostly False\n",
    "    'Street', 'Turning_Loop',\n",
    "\n",
    "    # High Missing Values\n",
    "    'End_Lat', 'End_Lng'\n",
    "]\n",
    "df_raw.drop(columns=cols_to_delete, errors='ignore', inplace=True)\n",
    "print(f\"\u00ad\u0192\u00d7\u2018 Dropped {len(cols_to_delete)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f36a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# RENAME COLUMNS\n",
    "# ---------------------------------------------------------\n",
    "df_raw.rename(columns={\n",
    "    'Distance(mi)': 'Distance',\n",
    "    'Temperature(F)': 'Temp',\n",
    "    'Humidity(%)': 'Humidity',\n",
    "    'Visibility(mi)': 'Visibility',\n",
    "    'Wind_Speed(mph)': 'Wind_Speed',\n",
    "    'Weather_Condition': 'Weather'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FIX DATE & TIME\n",
    "# ---------------------------------------------------------\n",
    "print(\"\u00ad\u0192\u00d7\u2019 Converting timestamps and calculating duration...\")\n",
    "df_raw['Start_Time'] = pd.to_datetime(df_raw['Start_Time'], errors='coerce')\n",
    "df_raw['End_Time'] = pd.to_datetime(df_raw['End_Time'], errors='coerce')\n",
    "\n",
    "# Drop rows where Start_Time or End_Time is unknown (cannot analyze without time)\n",
    "df_raw.dropna(subset=['Start_Time', 'End_Time'], inplace=True)\n",
    "\n",
    "# Create \"Duration\" (in Minutes)\n",
    "df_raw['Duration'] = (df_raw['End_Time'] - df_raw['Start_Time']).dt.total_seconds() / 60\n",
    "\n",
    "# Filter logic: Remove negative durations or accidents lasting > 1 week (bad data)\n",
    "df_raw = df_raw[(df_raw['Duration'] > 0) & (df_raw['Duration'] < 10080)]\n",
    "\n",
    "# Extract Temporal Features\n",
    "df_raw['Year'] = df_raw['Start_Time'].dt.year\n",
    "df_raw['Month'] = df_raw['Start_Time'].dt.month\n",
    "df_raw['Hour'] = df_raw['Start_Time'].dt.hour\n",
    "df_raw['Weekday'] = df_raw['Start_Time'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FIX MISSING VALUES (IMPUTATION)\n",
    "# ---------------------------------------------------------\n",
    "print(\"\u00ad\u0192\u00a7\u00a7 Filling empty values...\")\n",
    "# A. Numerical Columns -> Fill with MEDIAN\n",
    "weather_nums = ['Temp', 'Humidity', 'Visibility', 'Wind_Speed']\n",
    "for col in weather_nums:\n",
    "    if col in df_raw.columns:\n",
    "        median_val = df_raw[col].median()\n",
    "        df_raw[col] = df_raw[col].fillna(median_val)\n",
    "\n",
    "# B. Categorical Columns -> Fill with MODE (Most Frequent)\n",
    "categorical_cols = ['Weather', 'Sunrise_Sunset', 'City']\n",
    "for col in categorical_cols:\n",
    "    if col in df_raw.columns:\n",
    "        if not df_raw[col].mode().empty:\n",
    "            mode_val = df_raw[col].mode()[0]\n",
    "            df_raw[col] = df_raw[col].fillna(mode_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938031ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# FINAL STATUS CHECK\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\u2705 DATA PIPELINE COMPLETE!\")\n",
    "print(f\"Final Shape: {df_raw.shape}\")\n",
    "print(f\"Any missing values left? {df_raw.isna().sum().sum()}\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Columns ready for analysis:\", list(df_raw.columns))\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33712200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_csv(\"cleaned_accidents.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e20aa",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# \u00ad\u0192\u00f2\u00c5 GENERAL EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ==============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3299083",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ba083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. TARGET VARIABLE: How bad are the accidents? (Severity)\n",
    "# ---------------------------------------------------------\n",
    "plt.subplot(2, 2, 1)\n",
    "ax = sns.countplot(x='Severity', data=df_raw, palette='viridis')\n",
    "plt.title(\"Distribution of Accident Severity\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Severity Level (1-4)\")\n",
    "plt.ylabel(\"Count of Accidents\")\n",
    "# Add count labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + 0.3, p.get_height() + 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d31347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. TEMPORAL: When do accidents happen? (By Hour)\n",
    "# ---------------------------------------------------------\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df_raw['Hour'], bins=24, kde=True, color='orange')\n",
    "plt.title(\"Accidents by Hour of Day\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Hour (0-23)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(range(0, 24))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37087a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. GEOSPATIAL: Which states are most dangerous? (Top 10)\n",
    "# ---------------------------------------------------------\n",
    "plt.subplot(2, 2, 3)\n",
    "top_states = df_raw['State'].value_counts().head(10)\n",
    "sns.barplot(x=top_states.values, y=top_states.index, palette='magma')\n",
    "plt.title(\"Top 10 States by Accident Count\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Number of Accidents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061498d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e75046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4.CORRELATION: What numeric variables move together?\n",
    "# ---------------------------------------------------------\n",
    "plt.subplot(2, 2, 4)\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = ['Severity', 'Distance', 'Temp', 'Humidity', 'Visibility', 'Wind_Speed', 'Duration']\n",
    "corr_matrix = df_raw[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (Numeric Variables)\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03756dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "df = pd.read_csv(\"cleaned_accidents.csv\")\n",
    "\n",
    "common_cols = [\n",
    "    'Severity','Duration','Distance',\n",
    "    'Start_Time','End_Time',\n",
    "    'Year','Month','Hour','Weekday',\n",
    "    'State'\n",
    "]\n",
    "branch_specs = {\n",
    "    \"weather\":  ['Weather'],\n",
    "    \"physics\":  ['Temp','Humidity','Visibility','Wind_Speed'],\n",
    "    \"signals\":  ['Traffic_Signal','Stop','Give_Way','Traffic_Calming'],\n",
    "    \"roads\":    ['Amenity','Bump','Crossing','Junction','No_Exit','Railway','Roundabout','Station'],\n",
    "    \"time\":     ['Sunrise_Sunset'],\n",
    "    \"location\": ['City','Start_Lat','Start_Lng']\n",
    "}\n",
    "\n",
    "out_dir = Path(\"dist\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "logging.info(\"Generating compressed Parquet files...\")\n",
    "\n",
    "for name, spec in branch_specs.items():\n",
    "    # deterministic column order: common_cols then spec items not in common\n",
    "    final_cols = common_cols + [c for c in spec if c not in common_cols]\n",
    "    # keep only columns that actually exist in df\n",
    "    cols_present = [c for c in final_cols if c in df.columns]\n",
    "    if not cols_present:\n",
    "        logging.warning(\"No columns for %s \u2014 skipping\", name)\n",
    "        continue\n",
    "\n",
    "    df_mini = df.loc[:, cols_present].copy()\n",
    "    out_path = out_dir / f\"data_{name}.parquet\"\n",
    "\n",
    "    try:\n",
    "        df_mini.to_parquet(out_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "        size_mb = out_path.stat().st_size / (1024*1024)\n",
    "        logging.info(\"\u2705 %s | %d cols | %.2f MB\", out_path, len(cols_present), size_mb)\n",
    "    except ImportError:\n",
    "        csv_fallback = out_path.with_suffix(\".csv\")\n",
    "        df_mini.to_csv(csv_fallback, index=False)\n",
    "        logging.warning(\"pyarrow not available \u2014 wrote CSV %s\", csv_fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# \u00ad\u0192\u00f2\u00be FINAL SOLUTION: LEGACY PARQUET (Small & Compatible)\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"data/cleaned_accidents.csv\")\n",
    "\n",
    "\n",
    "# Assumption: You have the main cleaned 'df' loaded in memory\n",
    "if 'df' not in locals():\n",
    "    print(\"\u274c ERROR: Load your main 'df' first!\")\n",
    "else:\n",
    "    print(\"\u00ad\u0192\u00dc\u00c7 Generating Legacy Parquet Files...\")\n",
    "    os.makedirs('paquets', exist_ok=True)\n",
    "\n",
    "    branch_specs = {\n",
    "        \"weather\":  ['Weather'],\n",
    "        \"physics\":  ['Temp', 'Humidity', 'Visibility', 'Wind_Speed'],\n",
    "        \"signals\":  ['Traffic_Signal', 'Stop', 'Give_Way', 'Traffic_Calming'],\n",
    "        \"roads\":    ['Amenity', 'Bump', 'Crossing', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station'],\n",
    "        \"time\":     ['Sunrise_Sunset'], \n",
    "        \"location\": ['City', 'Start_Lat', 'Start_Lng']\n",
    "    }\n",
    "    \n",
    "    common_cols = ['Severity', 'Duration', 'Distance', 'Start_Time', 'End_Time', 'Year', 'Month', 'Hour', 'Weekday', 'State']\n",
    "\n",
    "    for member, specific_cols in branch_specs.items():\n",
    "        final_cols = list(set(common_cols + specific_cols))\n",
    "        available_cols = [c for c in final_cols if c in df.columns]\n",
    "        \n",
    "        # \u26a0\ufe0f THE MAGIC FIX: version='1.0'\n",
    "        # This makes files compatible with ALL readers (fastparquet & pyarrow)\n",
    "        filename = f\"paquets/data_{member}.parquet\"\n",
    "        \n",
    "        df[available_cols].to_parquet(\n",
    "            filename, \n",
    "            index=False, \n",
    "            engine='pyarrow', \n",
    "            compression='gzip', \n",
    "            version='1.0'  # <--- This fixes the \"Histogram\" error\n",
    "        )\n",
    "        \n",
    "        # Check size (Should be small now, ~30MB)\n",
    "        size_mb = os.path.getsize(filename) / (1024 * 1024)\n",
    "        print(f\"\u00ad\u0192\u00a3\u00e0 Created: {filename} | Size: {size_mb:.2f} MB\")\n",
    "\n",
    "    print(\"\\n\u00ad\u0192\u00c4\u00eb Done. These will fit on GitHub easily.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}